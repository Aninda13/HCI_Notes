# Lesson 19

This lesson will cover the following topics:

1. **Evaluation** - where we take what we've designed and put it in front of users to get their feedback
2. **Qualitative evaluation** - e.g., what user's like and don't like
3. **Empirical evaluation** - e.g., whether the time to complete a task has changed
4. **Predictive evaluation** - predicting what the results of user evaluation will be

## Three Types Of Evaluation

- **Qualitative evaluation** - evaluation that emphasizes the totality of a phenomenon
- **Empirical evaluation** - evaluation based on numeric summaries or observations of a phenomenon
- **Predictive evaluation** - evaluation based on systematic application of pre-established principles and heuristics

## Evaluation Terminology

- **Reliability** - whether a measure consistently returns the same results for the same phenomenon
- **Validity** - whether a measure's results actually reflect the underlying phenomenon
- **Generalizability** - whether a measure's results can be used to predict phenomena beyond what it measured
- **Precision** - the level of detail a measure supplies

## 5 Tips: What To Evaluate

1. Efficiency - how long does user take to achieve text (Expert)
2. Accuracy - how many users does users commit while executing a task (Expert)
3. Learnability - How long does user take to reach expertise
4. Memorability - users abiility to use interface over time
5. Satisfaction - Cognitive load, how many actually download the app?

## Evaluation Timeline

The evaluation timeline usually is as follows:

Regarding purpose:

1. **Formative** - primary purpose is to help redesign and improve our interface
2. **Summative** - the intention of conclusively saying at the end what the difference was

Regarding approach: Ways to fullfill purpose

1. Qualitative - the goal is to help us improve and understand tasks
2. Predictive - inform how we revise and improve our interface over time
3. Empirical - the goal is to demonstrate or assess change

Regarding data:

1. Qualitative - always useful to improve our interfaces
2. Quantitative - while always useful, can only arise when we have rigorous evaluations

Regarding setting: where does it take place.

1. Lab testing - helps us focus exclusively on the interface early on
2. Field testing - helps us focus more on the interface in context

## Evaluation Design

1. Define the task - very large or very small task.
2. Define performance measures - how are we going to measure this.
3. Develop the experiment - How we find user performance on the measures.
4. Recruit participants - Ethics
5. Do the experiment
6. Analyze the data - what data tells about performance measures.
7. Summarize the data - Informs ongoing process

## Qualitative Evaluation

There are some questions we want to ask in this evaluation: (Similar to interviews)

1. What did you like/dislike?
2. What were you thinking while using this interface?
3. What was your goal when you took that particular action?

## Designing A Qualitative Evaluation

There are options when designing a qualitative evaluation:

1. Prior experience or live demonstration? - bring user in to test
2. Synchronous or asynchronous? - watch live or complete and send
3. One interface or multiple prototypes? - Vary the order based on bias.
4. Think aloud protocol or post-event protocol? - explain while doing or do later at the end.
5. Individuals or groups? - Focus groups (build and expand)/ Only source of knowledge but no bias.

## Capturing Qualitative Evaluation

Options to capture qualitative evaluation:

1. Video recording 
	- Pros; Automated Comprehensive and passive/ 
	- Cons: intrusive, nonaalyzalble and screenless. Overwhelming.
2. Note-taking 
	- Pros: Cheap, Non intrusive and Analyzable/ 
	- Cons: Slow, Manual and Limited
3. Software logging
	- Pros: Automated passive and analyzable
	- Cons: Limited, Narrow and Tech Senistive

## 5 Tips: Qualitative Evaluation

1. Run pilot studies - Recruiting is hard, gather usefull data . Use friends and coworkers
2. Focus on feedback - Dont explain rationale, dont teach. Take it and design.
3. Use questions - when user get stuck? Guide user
4. Instruct users what to do, not how - Reduce bias
5. Capture satisfaction - Do they like it?

## Empirical Evaluation

The goal of empirical evaluation is to come up with strong conclusions. Most empirical evaluations are comparisons.

## Designing Empirical Evaluation

- **Treatment** - what a participant does in an experiment
- **Between subjects design** - comparison between two groups of subjects receiving different treatments
- **Within subjects design** - comparison within one group experiencing multiple treatments
- **Random assignment** - using random chance to decide what treatment each participant receives

## Hypothesis Testing

- **Hypothesis testing** - testing whether or not the data allows us to conclude a difference exists

## Quantitative Data And Empirical Tests

Recall that there are a number of tests for quantitative data:

1. Nominal:
   - Recommended - Chi-squared test
   - Alternatively: Fisher's exact test, G-test
2. Ordinal:
   - Recommended - Kolmogorov-Smirnov test
   - Alternatively - Chi-squared test, median test
3. Interval and ratio:
   - Recommended - Student's _t_-test
   - Alternatively - MWW test, Kruskal-Wallis test

## Summary Of Empirical Tests

Below is a summary of empirical tests:

![Summary of Empirical Tests](lesson-19-summary-of-empirical-tests.JPG)

## 5 Tips: Empirical Evaluation

1. Control what you can, document what you can't
2. Limit your variables
3. Work backwards
4. Script your analyses in advance
5. Pay attention to power

## Predictive Evaluation

Predictive evaluation should only be used where we wouldn't otherwise be doing any evaluation.

## Types Of Predictive Evaluation

- **Heuristic evaluation** - each individual evaluator inspects the interface alone, and identifies places where the interface violates some heuristic
- **Model-based evaluation** - tracing through models in the context of the interface we designed (e.g., GOMS model)
- **Simulation-based evaluation** - where we might construct an AI agent that interacts with our interface in the way a human would

## Cognitive Walkthrough

The most common type of predictive evaluation is actually cognitive walkthrough:

- **Cognitive walkthrough** - stepping through the process of interacting with an interface, mentally simulating in each stage what the user is seeing and thinking and doing

## Evaluating Prototypes

Our goal is to constantly apply multiple evaluation techniques to center our designs on the user.

## Important Videos